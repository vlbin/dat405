{"cells":[{"cell_type":"markdown","source":"# DAT405 Introduction to Data Science and AI \n## Assignment 4: Spam classification using Naïve Bayes \n\nThe exercise takes place in a notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \nHints:\nYou can execute certain linux shell commands by prefixing the command with `!`. You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results the second you can use writing code snippets that execute the tasks required.  \n\nIn this assignment you will implement a Naïve Bayes classifier in Python that will classify emails into spam and non-spam (“ham”) classes.  Your program should be able to train on a given set of spam and “ham” datasets. \nYou will work with the datasets available at https://spamassassin.apache.org/old/publiccorpus/. There are three types of files in this location: \n-\teasy-ham: non-spam messages typically quite easy to differentiate from spam messages. \n-\thard-ham: non-spam messages more difficult to differentiate \n-\tspam: spam messages \n\n**Execute the cell below to download and extract the data into the environment of the notebook -- it will take a few seconds.** If you chose to use Jupyter notebooks you will have to run the commands in the cell below on your local computer, with Windows you can use 7zip (https://www.7-zip.org/download.html) to decompress the data.\n\n","metadata":{"id":"-sTsDfIVKsmL","cell_id":"00000-aa9823a3-08d0-4f6c-a93b-70e119a8f2f3","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"Wa37fBwRF-xe","cell_id":"00001-2de61efa-ba3b-47d1-935e-6d3afc7bdb4d","deepnote_to_be_reexecuted":false,"source_hash":"318bae5f","execution_millis":0,"execution_start":1613583206105,"deepnote_cell_type":"code"},"source":"#Download and extract data\n#!wget https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n#!wget https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n#!wget https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n#!tar -xjf 20021010_easy_ham.tar.bz2\n#!tar -xjf 20021010_hard_ham.tar.bz2\n#!tar -xjf 20021010_spam.tar.bz2","execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"*The* data is now in the three folders `easy_ham`, `hard_ham`, and `spam`.","metadata":{"id":"tdH1XTepLjZ3","cell_id":"00002-d8cd0f97-5e98-487c-974c-0e6b9ebd8cdc","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"A53Gw00fBLG2","cell_id":"00003-62f648df-6370-4997-84b8-65a08c9bb00f","deepnote_to_be_reexecuted":false,"source_hash":"9072a7c1","execution_millis":790,"execution_start":1613583206153,"deepnote_cell_type":"code"},"source":"!ls -lah","execution_count":2,"outputs":[{"name":"stdout","text":"total 4.5M\r\ndrwxrwxr-x 5 root root    9 Feb 17 17:33 .\r\ndrwxr-xr-x 8 root root    8 Feb 10 11:15 ..\r\n-rw-r--r-- 1 root root 1.6M Jun 29  2004 20021010_easy_ham.tar.bz2\r\n-rw-r--r-- 1 root root 998K Dec 16  2004 20021010_hard_ham.tar.bz2\r\n-rw-r--r-- 1 root root 1.2M Jun 29  2004 20021010_spam.tar.bz2\r\n-rw-rw-r-- 1 root root  35K Feb 17 17:33 Assignment4.ipynb\r\ndrwx--x--x 2  500  500 2.5K Oct 10  2002 easy_ham\r\ndrwx--x--x 2 1000 1000  252 Dec 16  2004 hard_ham\r\ndrwxr-xr-x 2  500  500  503 Oct 10  2002 spam\r\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 1. Preprocessing: \n1.\tNote that the email files contain a lot of extra information, besides the actual message. Ignore that and run on the entire text. \n2.\tWe don’t want to train and test on the same data. Split the spam and the ham datasets in a training set and a test set. (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`) **0.5p**\n","metadata":{"id":"DGlWPVnSNzT7","cell_id":"00004-e4d37016-685c-4054-8eba-c0c26fdb121e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"J2sllUWXKblD","cell_id":"00005-787273d0-f2a7-42dd-90b7-d1e06f1f2142","deepnote_to_be_reexecuted":false,"source_hash":"7ce570f5","execution_millis":2535,"execution_start":1613583206910,"deepnote_cell_type":"code"},"source":"import os\nfrom sklearn.model_selection import train_test_split\n\ndef load_files(dir):\n    files = []\n    with os.scandir(dir) as it:\n        for entry in it:\n            with open(entry, 'r',  encoding='iso-8859-1') as f:\n                data = f.read()\n                files.append(data)\n    return files\n\neasy_ham = load_files('easy_ham')\nhard_ham = load_files('hard_ham')\nspam = load_files('spam')\n\n\nham_label = ['ham']*(len(easy_ham + hard_ham))\nspam_label = ['spam']*len(spam)\n\nham_train, ham_test, ham_label_train, ham_label_test = train_test_split(easy_ham + hard_ham, ham_label, test_size=0.3, random_state=42)\nspam_train, spam_test, spam_label_train, spam_label_test = train_test_split(spam, spam_label, test_size=0.3, random_state=42)\n\n\nhard_ham_label = ['hardham']*(len(hard_ham))\neasy_ham_label = ['easyham']*len(easy_ham)\n\nhard_ham_train, hard_ham_test, hard_ham_label_train, hard_ham_label_test = train_test_split(hard_ham, hard_ham_label, test_size=0.3, random_state=42)\neasy_ham_train, easy_ham_test, easy_ham_label_train, easy_ham_label_test = train_test_split(easy_ham, easy_ham_label, test_size=0.3, random_state=42)","execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### 2. Write a Python program that: \n1.\tUses four datasets (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`) \n2.\tTrains a Naïve Bayes classifier (e.g. Sklearn) on `hamtrain` and `spamtrain`, that classifies the test sets and reports True Positive and True Negative rates on the `hamtest` and `spamtest` datasets. You can use `CountVectorizer` to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifier in SKlearn ([Documentation here](https://scikit-learn.org/stable/modules/naive_bayes.html)). Test two of these classifiers that are well suited for this problem\n    - Multinomial Naive Bayes  \n    - Bernoulli Naive Bayes. \n\n\n\n\n","metadata":{"id":"mnbrbI0_OKCF","cell_id":"00006-ae3d4316-6711-4075-b85e-0898168ff535","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"MJERHSCcGNaW","cell_id":"00007-8c3533a2-f4df-45e9-88f9-ffc3db41b8a4","deepnote_to_be_reexecuted":false,"source_hash":"f3f51f61","execution_millis":149,"execution_start":1613583209446,"deepnote_cell_type":"code"},"source":"#Imports\nfrom sklearn import datasets\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nfrom pprint import pprint\n\n\n\ndef run_predictions(hamtrain, spamtrain, hamtest, spamtest,\n                    hamtrainlabels, spamtrainlabels, hamtestlabels, spamtestlabels,\n                    mnb=MultinomialNB(), \n                    bnb=BernoulliNB(), \n                    vectorizer=CountVectorizer(),\n                    return_accuracy=False):\n\n    #Train the model using the training sets\n    train = vectorizer.fit_transform(hamtrain + spamtrain)\n\n    test_ham = vectorizer.transform(hamtest)\n    test_spam = vectorizer.transform(spamtest)\n    test_set = vectorizer.transform(hamtest + spamtest)\n\n    #Predict the response for test dataset\n    mnb.fit(train, hamtrainlabels + spamtrainlabels)\n    bnb.fit(train, hamtrainlabels + spamtrainlabels)\n\n    pred_mnb_ham = mnb.predict(test_ham)\n    pred_bnb_ham = bnb.predict(test_ham)\n\n    pred_mnb_spam = mnb.predict(test_spam)\n    pred_bnb_spam = bnb.predict(test_spam)\n\n    if return_accuracy:\n        return (mnb.score(test_set, hamtestlabels + spamtestlabels), bnb.score(test_set, hamtestlabels + spamtestlabels))\n    # Show\n    unique, counts = np.unique(pred_mnb_ham, return_counts=True)\n    print(f\"True positives (Multinomial): {dict(zip(unique, counts))[hamtestlabels[0]]}\")\n    unique, counts = np.unique(pred_bnb_ham, return_counts=True)\n    print(f\"True positives (Bernoulli):   {dict(zip(unique, counts))[hamtestlabels[0]]}\")\n\n    print()\n\n    unique, counts = np.unique(pred_mnb_spam, return_counts=True)\n    print(f\"True negatives (Multinomial): {dict(zip(unique, counts))[spamtestlabels[0]]}\")\n    unique, counts = np.unique(pred_bnb_spam, return_counts=True)\n    print(f\"True negatives (Bernoulli):   {dict(zip(unique, counts))[spamtestlabels[0]]}\")\n\n    print()\n\n    print(f\"Accuracy for multinomial test: {mnb.score(test_set, hamtestlabels + spamtestlabels):,.2f}\")\n    print(f\"Accuracy for bernoulli test: {bnb.score(test_set, hamtestlabels + spamtestlabels):,.2f}\")\n\n    print()","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00008-901c8ed3-be1d-4de1-a6c2-b9c9d0f97866","deepnote_to_be_reexecuted":false,"source_hash":"f0af6e7a","execution_millis":4550,"execution_start":1613583209626,"deepnote_cell_type":"code"},"source":"run_predictions(ham_train, spam_train, ham_test, spam_test, ham_label_train, spam_label_train, ham_label_test, spam_label_test)","execution_count":5,"outputs":[{"name":"stdout","text":"True positives (Multinomial): 840\nTrue positives (Bernoulli):   839\n\nTrue negatives (Multinomial): 131\nTrue negatives (Bernoulli):   30\n\nAccuracy for multinomial test: 0.98\nAccuracy for bernoulli test: 0.88\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"a) Explain how the classifiers differ. What different interpretations do they have? **1p** ","metadata":{"cell_id":"00008-b62b693a-fa94-4b62-942f-e3a45e229cd9","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"The classifiers used above are Multinomial and Bernoulli Naive Bayes, the main difference between the two is that the multinomial classifier is based on the occurence of discrete features, e.g. something that can undertake any value - let's say the number of spam-related words in an email. The more frequent that feature is in the email, the more likely it is to be considered spam. The Bernoulli classifier, however, only takes into account if a feature is present or not - in our case a spam-related word/formulation - not how frequently this feature occurs.\n\nAs shown in the prints above the multinomial classifier is more accurate, this can thus be explained by the fact that it doesn't only consider whether a feature is present or not, but how frequent it is. \n\nFor (a very simplified) example, an email that is spam might be a money-related scam, so the email might include many dollar signs. The multinomial classifier realises that it is a lot more frequent in spam emails than ham emails - but the dollar sign can of course still be present in a ham email without being in a malicious context. The Bernoulli classifier cannot consider this, as it will only check whether the dollar sign is present or not in the email.","metadata":{"id":"nI1bPDCvQxen","cell_id":"00009-3056c0ba-af4e-4fba-9a41-79c4d29ee3e1","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### 3.Run your program on \n-\tSpam versus easy-ham \n-\tSpam versus (hard-ham + easy-ham). \n-   Discuss your results **2.5p** ","metadata":{"id":"wDFS3uFFUcS7","cell_id":"00010-854d455e-1edc-4ac7-ba6a-1108546562ff","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"** Spam versus easy-ham **","metadata":{"tags":[],"cell_id":"00012-90fb0a91-8e32-4328-8b28-4e3488611de3","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"gool_zb8Qzzy","cell_id":"00011-daa035bf-867a-46ce-a0b2-16e25dd30df0","deepnote_to_be_reexecuted":false,"source_hash":"d439ba49","execution_millis":3116,"execution_start":1613583214192,"deepnote_cell_type":"code"},"source":"run_predictions(easy_ham_train, spam_train, easy_ham_test, spam_test, easy_ham_label_train, spam_label_train, easy_ham_label_test, spam_label_test)","execution_count":6,"outputs":[{"name":"stdout","text":"True positives (Multinomial): 766\nTrue positives (Bernoulli):   764\n\nTrue negatives (Multinomial): 129\nTrue negatives (Bernoulli):   69\n\nAccuracy for multinomial test: 0.98\nAccuracy for bernoulli test: 0.91\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"** Spam versus (hard-ham + easy-ham). **","metadata":{"tags":[],"cell_id":"00013-31d79c88-bb2e-4b52-9b61-8e3e5173d079","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00013-53baf0bc-d742-4701-a74d-5a23e7656683","deepnote_to_be_reexecuted":false,"source_hash":"4b08ec8e","execution_millis":4427,"execution_start":1613583217300,"deepnote_cell_type":"code"},"source":"#same set as question 2\nrun_predictions(ham_train, spam_train, ham_test, spam_test, ham_label_train, spam_label_train, ham_label_test, spam_label_test)","execution_count":7,"outputs":[{"name":"stdout","text":"True positives (Multinomial): 840\nTrue positives (Bernoulli):   839\n\nTrue negatives (Multinomial): 131\nTrue negatives (Bernoulli):   30\n\nAccuracy for multinomial test: 0.98\nAccuracy for bernoulli test: 0.88\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"There is not much of a difference between the two looking on the accuracy. However the spam prediction with the Bernoulli classifier finds more than twice as many spam-emails in the easy-ham set compared to the combined set. As previously stated, this is likely because it doesn't take into account the **frequency** of the feature, only whether the spam-related word is present or not. The hard-ham emails might have more spam-like features without actually being spam, thus it becomes more necessary to also consider the frequency of the spam-related words.","metadata":{"tags":[],"cell_id":"00016-94988f8a-d311-4107-9a7c-8c332bcca8c6","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# 4.\tTo avoid classification based on common and uninformative words it is common to filter these out. \n\n**a.** Argue why this may be useful. Try finding the words that are too common/uncommon in the dataset. **1p** ","metadata":{"id":"TkfQWBB4UhYd","cell_id":"00012-f54e22ad-727a-4a73-9cfe-d346b94ae37d","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00018-f60aee65-d258-49ba-9d69-8e180717afe7","deepnote_to_be_reexecuted":false,"source_hash":"d1cebd8d","execution_millis":890,"execution_start":1613583221724,"deepnote_cell_type":"code"},"source":"from collections import Counter \nfrom collections import OrderedDict\n\n\nham = easy_ham + hard_ham\n\n\n\ndef get_most_uncommon_common(dataset, low, high):\n    all_common = dict()\n    for email in dataset:\n        words = email.split()\n        counter = Counter(words)\n        most_occur = counter.most_common(10)\n        for (w, i) in most_occur:\n            if not w.isalpha():\n                continue\n            if w in all_common:\n                all_common[w] += i\n            else:\n                all_common[w] = i\n\n    d_descending = OrderedDict(sorted(all_common.items(), \n                                    key=lambda kv: kv[1], reverse=True))\n    cleaned = {key:val for key, val in dict(d_descending).items() if val < low or val > high}\n    return cleaned\n\nprint('Ham most uncommon and common: ' , get_most_uncommon_common(ham, 10, 2000))\nprint('Spam most uncommon and common: ' ,get_most_uncommon_common(spam, 10, 1000))","execution_count":8,"outputs":[{"name":"stdout","text":"Ham most uncommon and common:  {'the': 25655, 'to': 14412, 'from': 14309, 'with': 14193, 'for': 13702, 'by': 13575, 'of': 11293, 'and': 10213, 'a': 9710, 'Sep': 9249, 'Oct': 5312, 'in': 5090, 'Aug': 4959, 'id': 4024, 'is': 2570, 'that': 2446, 'I': 2244, 'SA': 9, 'bees': 9, 'Load': 9, 'player': 9, 'play': 9, 'military': 9, 'memory': 8, 'other': 8, 'training': 8, 'features': 8, 'XDegrees': 8, 'something': 8, 'write': 8, 'speech': 8, 'them': 8, 'files': 8, 'And': 8, 'themes': 8, 'WordInfo': 8, 'web': 8, 'spamd': 8, 'bytecode': 8, 'into': 7, 'yield': 7, 'percentages': 7, 'unique': 7, 'dot': 7, 'gif': 7, 'floor': 7, 'SubSection': 7, 'messages': 7, 'number': 7, 'Mom': 7, 'Judo': 7, 'clues': 7, 'Tried': 7, 'dequeue': 7, 'fetching': 7, 'object': 7, 'Solaris': 7, 'inbound': 7, 'CatchUp': 7, 'but': 6, 'MARLA': 6, 'same': 6, 'cab': 6, 'timtest': 6, 'File': 6, 'top': 6, 'once': 6, 'trained': 6, 'procmail': 6, 'Right': 6, 'show': 6, 'shows': 6, 'lizard': 6, 'store': 6, 'honor': 6, 'Bad': 6, 'razor': 6, 'RDF': 6, 'Gaim': 6, 'CNET': 6, 'PM': 6, 'That': 5, 'Safe': 5, 'email': 5, 'so': 5, 'MUA': 5, 'AmphetaDesk': 5, 'picked': 5, 'available': 5, 'SF': 5, 'headers': 5, 'built': 5, 'learn': 5, 'an': 5, 'setup': 5, 'toilet': 5, 'Disney': 5, 'recent': 5, 'Reg': 5, 'out': 5, 'MS': 5, 'server': 4, 'Cap': 4, 'one': 4, 'FOAF': 4, 'game': 4, 'brown': 4, 'urgent': 4, 'advertising': 4, 'real': 4, 'Barney': 4, 'San': 4, 'mobile': 4, 'Boing': 4, 'Jethro': 4, 'tokenizer': 4, 'Anthony': 4, 'Vocera': 4, 'Twinkies': 4, 'read': 4, 'results': 4, 'Razor': 4, 'may': 4, 'paper': 4, 'sets': 4, 'James': 4, 'Best': 4, 'very': 4, 'no': 4, 'click': 4, 'May': 4, 'its': 3, 'Sat': 3, 'only': 3, 'feature': 3, 'duplicates': 3, 'tagging': 3, 'Greg': 3, 'just': 3, 'being': 3, 'move': 3, 'database': 3, 'part': 3, 'due': 3, 'module': 3, 'currently': 3, 'got': 3, 'each': 3, 'different': 3, 'set': 3, 'know': 2, 'maybe': 2, 'subject': 2, 'Skip': 2, 'scripts': 2, 'Nigerian': 2, 'classifier': 2, 'corpus': 2, 'version': 2, 'before': 2, 'DCC': 2, 'CVS': 2, 'per': 2, 'Neale': 2, 'Baxter': 2, 'parts': 2, 'percentage': 2, 'right': 2, 'spambayes': 2, 'Mon': 1}\nSpam most uncommon and common:  {'the': 4375, 'to': 3682, 'for': 2720, 'and': 2651, 'of': 2635, 'from': 2049, 'with': 2040, 'by': 1761, 'you': 1683, 'Sep': 1379, 'a': 1241, 'in': 1060, 'Times': 9, 'insurance': 9, 'How': 9, 'many': 9, 'ME': 9, 'Laptop': 9, 'Computer': 9, 'Ref': 9, 'Intel': 9, 'Processor': 9, 'RAM': 9, 'life': 8, 'Click': 8, 'LTC': 8, 'we': 8, 'web': 7, 'MLM': 7, 'EIN': 7, 'number': 7, 'This': 7, 'make': 6, 'Internet': 6, 'Ou': 6, 'Wei': 6, 'own': 5, 'MT': 5, 'ACCOUNT': 5, 'Euro': 5, 'at': 5, 'RMB': 5, 'AM': 4, 'PM': 4, 'unknown': 4, 'Phone': 4, 'As': 4, 'poker': 4, 'Chinese': 4, 'Du': 4, 'aus': 4, 'China': 4, 'per': 4, 'test': 4, 'drive': 4, 'die': 3, 'der': 3, 'Microsoft': 3, 'Access': 3, 'Wed': 1, 'Sat': 1}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In the lists above you can see the most common and uncommon words for both ham and spam. The boundaries used are if a word appears more than 2000 times in the ham set or 1000 times in the spam set its very common and less then 10 times its very uncommon. We took a lower upperboundry for the spam set as the spam set contains fewer emails.\n***","metadata":{"tags":[],"cell_id":"00019-dd9f4f3f-11e5-44f7-ac65-8730df689d04","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**b.** Use the parameters in Sklearn’s `CountVectorizer` to filter out these words. Update the program from point 3 and run it on your data and report and discuss your results. You have two options to do this in Sklearn: either using the words found in part (a) or letting Sklearn do it for you. Argue for your decision-making. **1p** ","metadata":{"tags":[],"cell_id":"00018-9636c7c4-5aa4-4f8d-a71e-cb8b8795f27d","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"qt7ELzEqUfas","cell_id":"00013-cfb32d00-2d7d-4b1a-9dd0-544856afb8bd","deepnote_to_be_reexecuted":false,"source_hash":"c8ae521e","execution_millis":65277,"execution_start":1613583222653,"deepnote_cell_type":"code"},"source":"#stopwords = list(set(list(get_noise(ham_train, 100, 2000).keys()) + list(get_noise(spam_train, 100, 500).keys())))\n#stopwords = [x.lower() for x in stopwords]\n#run_predictions(easy_ham_train, spam_train, easy_ham_test, spam_test, easy_ham_label_train, spam_label_train, easy_ham_label_test, spam_label_test)\n\n\nlow = [0.01, 0.02, 0.04, 0.08]\nhigh = [0.9, 0.92, 0.94, 0.98]\n\nbest_accuracy = ()\nbest_values = tuple()\n\nfor i in low:\n    for k in high:\n        result = run_predictions(ham_train, spam_train, ham_test, spam_test, ham_label_train, spam_label_train, ham_label_test, spam_label_test, vectorizer=CountVectorizer(min_df=i, max_df = k), return_accuracy=True)\n        print('low:', i, 'high:', k, 'multinomial:', round(result[0], 2), 'binomial:', round(result[1], 2))","execution_count":9,"outputs":[{"name":"stdout","text":"low: 0.01 high: 0.9 multinomial: 0.98 binomial: 0.98\nlow: 0.01 high: 0.92 multinomial: 0.98 binomial: 0.98\nlow: 0.01 high: 0.94 multinomial: 0.98 binomial: 0.98\nlow: 0.01 high: 0.98 multinomial: 0.98 binomial: 0.98\nlow: 0.02 high: 0.9 multinomial: 0.97 binomial: 0.97\nlow: 0.02 high: 0.92 multinomial: 0.97 binomial: 0.97\nlow: 0.02 high: 0.94 multinomial: 0.97 binomial: 0.97\nlow: 0.02 high: 0.98 multinomial: 0.97 binomial: 0.97\nlow: 0.04 high: 0.9 multinomial: 0.95 binomial: 0.93\nlow: 0.04 high: 0.92 multinomial: 0.95 binomial: 0.93\nlow: 0.04 high: 0.94 multinomial: 0.95 binomial: 0.93\nlow: 0.04 high: 0.98 multinomial: 0.95 binomial: 0.94\nlow: 0.08 high: 0.9 multinomial: 0.94 binomial: 0.9\nlow: 0.08 high: 0.92 multinomial: 0.94 binomial: 0.9\nlow: 0.08 high: 0.94 multinomial: 0.95 binomial: 0.91\nlow: 0.08 high: 0.98 multinomial: 0.94 binomial: 0.91\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We decided to use SkLearns own defined parameters when filtering out words to minimize the human factor in the result. To get a good match for the values\nwe tried a few different `min_df` and `max_df` values. Doing this we found that using \"harsher\" values (e.g. 0.1 and 0.9+) produced the best results - see above. ","metadata":{"tags":[],"cell_id":"00021-5cc257aa-dfcd-4d45-b8b9-77899175528b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### 5. Eeking out further performance\n**a.**  Use a lemmatizer to normalize the text (for example from the `nltk` library). For one implementation look at the documentation ([here](https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes)). Run your program again and answer the following questions: \n  - Why can lemmatization help?\n  -\tDoes the result improve from 3 and 4? Discuss. **1.5p** \n\n\n\n\n\n","metadata":{"id":"zcyVfOZFU4F_","cell_id":"00014-37156bab-a220-4805-9b92-ce2944a4f690","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import nltk\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nnltk.download('punkt')\nnltk.download('wordnet')","metadata":{"tags":[],"cell_id":"00024-6284a2ed-bdda-4f21-8f45-325d74e583ba","deepnote_to_be_reexecuted":false,"source_hash":"18b1b1d5","execution_start":1613583287929,"execution_millis":532,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"True"},"metadata":{}}],"execution_count":10},{"cell_type":"code","metadata":{"cell_id":"00015-b0ecbd56-9d27-468d-ad77-bc890e7dadef","deepnote_to_be_reexecuted":false,"source_hash":"638458e9","execution_millis":138017,"execution_start":1613583288457,"deepnote_cell_type":"code"},"source":"class LemmaTokenizer:\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, doc):\n        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n\nvect1 = CountVectorizer(tokenizer=LemmaTokenizer())\nvect2 = CountVectorizer(tokenizer=LemmaTokenizer(), min_df=0.01, max_df=0.98)\n\nrun_predictions(ham_train, spam_train, ham_test, spam_test, ham_label_train, spam_label_train, ham_label_test, spam_label_test, vectorizer=vect1)\nrun_predictions(ham_train, spam_train, ham_test, spam_test, ham_label_train, spam_label_train, ham_label_test, spam_label_test, vectorizer=vect2)","execution_count":11,"outputs":[{"name":"stdout","text":"True positives (Multinomial): 839\nTrue positives (Bernoulli):   836\n\nTrue negatives (Multinomial): 126\nTrue negatives (Bernoulli):   51\n\nAccuracy for multinomial test: 0.97\nAccuracy for bernoulli test: 0.89\n\nTrue positives (Multinomial): 824\nTrue positives (Bernoulli):   813\n\nTrue negatives (Multinomial): 144\nTrue negatives (Bernoulli):   151\n\nAccuracy for multinomial test: 0.98\nAccuracy for bernoulli test: 0.97\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Why can lemmatization help?**\n\nLemmatization can help by grouping different implementations of words, let's take the word \"use\" for example, in the multinomial classification \nit might group together words like using, used, usage to be a single \"class\" of words, thus increasing the frequency as they are treated as the same word.\nFurthermore, in the Bernoulli classification it can improve since it checks multiple implementations of the word at the same time, thus increasing the probability\nthat a word class will be detected in multiple emails instead of the different words being treated by themselves.\n\n**Does the result improve from 3 and 4?**\n\nIn our case the result is not heavily improved, the only increase in performance is that the Bernoulli classification from 3 is improved by one percentage point. \n***","metadata":{"tags":[],"cell_id":"00026-b10ff59c-107a-4866-af8b-8a7bb86d2ba1","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**b.** The split of the data set into a training set and a test set can lead to very skewed results. Why is this, and do you have suggestions on remedies? \n What do you expect would happen if your training set were mostly spam messages while your test set were mostly ham messages?  **1p** ","metadata":{"cell_id":"00016-d536ca49-b7f4-4642-befa-19354c0bb338","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"The split can lead to skewed results if the proportions of spam versus ham messages are different in the training and test set, let's use the example from the question for reference. If the training set is mostly spam messages the model will be biased towards spam and not be able to produce a good interpretation of what a ham message looks like and the \"spam threshold\" will not be balanced. When the model is later tested, it will, as a consequence of this biased training, predict that many ham messages are spam as well.\n\nTo remedy this, we can split the datasets beforehand to make sure that the proportions of spam messages versus ham messages are (almost) the same in both the training set and test set. \n","metadata":{"tags":[],"cell_id":"00028-e793a3c1-1f63-4a75-85c7-af62ed485a7c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**c.** Re-estimate your classifier using `fit_prior` parameter set to `false`, and answer the following questions:\n  - What does this parameter mean?\n  - How does this alter the predictions? Discuss why or why not. **0.5p** ","metadata":{"id":"s_nyGug9U4f3","cell_id":"00018-a29661f0-661d-4c99-95e5-059c9ab48e9d","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"ham_prior = len(ham_train) / len(ham_train + spam_train)\nspam_prior = 1 - ham_prior\n\n\nprint('No prior provided')\nrun_predictions(ham_train, spam_train, ham_test, spam_test, ham_label_train, spam_label_train, ham_label_test, spam_label_test, mnb=MultinomialNB(fit_prior=False), bnb=BernoulliNB(fit_prior=False))\nprint('-------------------')\nprint('Prior provided')\nrun_predictions(ham_train, spam_train, ham_test, spam_test, ham_label_train, spam_label_train, ham_label_test, spam_label_test, mnb=MultinomialNB(fit_prior=True, class_prior=[ham_prior, spam_prior]), bnb=BernoulliNB(fit_prior=True, class_prior=[ham_prior, spam_prior]))","metadata":{"tags":[],"cell_id":"00030-b7397e57-432a-46ca-8263-516cc6f2eeba","deepnote_to_be_reexecuted":false,"source_hash":"2309e2d1","execution_millis":9291,"execution_start":1613583426474,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"No prior provided\nTrue positives (Multinomial): 840\nTrue positives (Bernoulli):   839\n\nTrue negatives (Multinomial): 132\nTrue negatives (Bernoulli):   31\n\nAccuracy for multinomial test: 0.98\nAccuracy for bernoulli test: 0.88\n\n-------------------\nPrior provided\nTrue positives (Multinomial): 840\nTrue positives (Bernoulli):   839\n\nTrue negatives (Multinomial): 131\nTrue negatives (Bernoulli):   30\n\nAccuracy for multinomial test: 0.98\nAccuracy for bernoulli test: 0.88\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"The fit_prior=False parameter means that the classifiers will use a uniform prior. This does barely alter the predictions, when prior is provided the true negatives get one less correct prediction. We believe that this might be because we use the built-in function for splitting data which uses the `random_state` parameter and\nshuffles the data before applying the split. If we did the splitting manually or without shuffling the data it might come in handy as the splitting otherwise could become skewed.\n***","metadata":{"tags":[],"cell_id":"00031-1f280049-9809-43bb-9236-f5e20681d64f","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**d.** The python model includes smoothing (`alpha` parameter ), explain why this can be important. \n  - What would happen if in the training data set the word 'money' only appears in spam examples? What would the model predict about a message containing the word 'money'? Does the prediction depend on the rest of the message and is that reasonable? Explain your reasoning  **1p** ","metadata":{"cell_id":"00020-5f74b604-05e9-48d1-957b-91dbfb7d08e0","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"If we don't use any smoothing the model would in theory instantly predict that a message containing 'money' is spam - no matter the other contents of the message - since there is no training messages containing 'money' that are ham. If we use the default value of 1, we produce a small \"wiggle room\" that a ham message can include 'money'. If the smoothing value gets unreasonably high the probability will move towards 50/50, which might not be accurate as we can assume that the word 'money' is more frequent in spam messages. When this happens we lose the value of the training set as the model now thinks that 'money' is as frequent in spam and ham messages. It is therefore a good idea to use a rather low value, such as 1. \n\n[Source](https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece) ","metadata":{"tags":[],"cell_id":"00033-1ede1706-3d39-49d7-a086-e7d48baf9d34","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### What to report and how to hand in.\n\n- You will need to clearly report all results in the notebook in a clear and appropriate way, either using plots or code output (f.x. \"print statements\"). \n- The notebook must be reproducible, that means, we must be able to use the `Run all` function from the `Runtime` menu and reproduce all your results. **Please check this before handing in.** \n- Save the notebook and share a link to the notebook (Press share in upper left corner, and use `Get link` option. **Please make sure to allow all with the link to open and edit.**\n- Edits made after submission deadline will be ignored, graders will recover the last saved version before deadline from the revisions history.\n- **Please make sure all cells are executed and all the output is clearly readable/visible to anybody opening the notebook.**","metadata":{"id":"ND6FKoexVAhW","cell_id":"00022-e9061155-537c-49c1-97dd-3d577af87b11","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"8bI3z_spVacz","cell_id":"00023-77fd4273-d4e2-4cf7-8612-575cbde2c898","deepnote_to_be_reexecuted":false,"source_hash":"b623e53d","execution_millis":23,"execution_start":1613583435774,"deepnote_cell_type":"code"},"source":"","execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4d60117c-244d-43ff-9bed-27d4239d5495' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":1,"metadata":{"colab":{"collapsed_sections":[],"name":"Assignment4.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"deepnote_notebook_id":"23947d9a-2514-4c9a-9ac7-f73111ab0227","deepnote":{},"deepnote_execution_queue":[]}}